
Creating model from DanQ()
Building the model
Compiling model

Creating model from DanQ()
Building the model
Compiling model
____________________________________________________________________________________________________
Layer (type)			 Output Shape	       Param #	   Connected to
====================================================================================================
convolution1d_1 (Convolution1D)  (None, 975, 320)      33600	   convolution1d_input_1[0][0]
____________________________________________________________________________________________________
maxpooling1d_1 (MaxPooling1D)	 (None, 75, 320)       0	   convolution1d_1[0][0]
____________________________________________________________________________________________________
dropout_1 (Dropout)		 (None, 75, 320)       0	   maxpooling1d_1[0][0]
____________________________________________________________________________________________________
flatten_1 (Flatten)		 (None, 24000)	       0	   dropout_1[0][0]
____________________________________________________________________________________________________
dense_1 (Dense) 		 (None, 925)	       22200925    flatten_1[0][0]
____________________________________________________________________________________________________
activation_1 (Activation)	 (None, 925)	       0	   dense_1[0][0]
____________________________________________________________________________________________________
dense_2 (Dense) 		 (None, 919)	       850994	   activation_1[0][0]
____________________________________________________________________________________________________
activation_2 (Activation)	 (None, 919)	       0	   dense_2[0][0]
====================================================================================================
Total params: 23,085,519
Trainable params: 23,085,519
Non-trainable params: 0
____________________________________________________________________________________________________
Saving models in json and yaml format to models/json/danq_4.json and  models/yaml/danq_4.yaml
Saving weights to models/weights/danq_4.hdf5 and epoch logs to models/csv/danq_4.csv
Retrieving train, validation, and test data

The date is 02/16/2017
The time is 01:27:29 AM

Loading weights from models/weights/danq_4.hdf5 if it exists
Run `tensorboard --logdir=models/run_logs/tensorboard` to open tensorboard at (default) 127.0.0.1:6006
Running at most 70 epochs
Train on 2200000 samples, validate on 8000 samples
Epoch 1/70
2199900/2200000 [============================>.] - ETA: 0ss--loss::0.08466--acc::0.9787Epoch 00000: val_loss improved from inf to 0.07639, saving model to models/weights/danq_4.hdf5

Creating model from DanQ()
Building the model
Compiling model
____________________________________________________________________________________________________
Layer (type)			 Output Shape	       Param #	   Connected to
====================================================================================================
convolution1d_1 (Convolution1D)  (None, 975, 320)      33600	   convolution1d_input_1[0][0]
____________________________________________________________________________________________________
maxpooling1d_1 (MaxPooling1D)	 (None, 75, 320)       0	   convolution1d_1[0][0]
____________________________________________________________________________________________________
dropout_1 (Dropout)		 (None, 75, 320)       0	   maxpooling1d_1[0][0]
____________________________________________________________________________________________________
flatten_1 (Flatten)		 (None, 24000)	       0	   dropout_1[0][0]
____________________________________________________________________________________________________
dense_1 (Dense) 		 (None, 925)	       22200925    flatten_1[0][0]
____________________________________________________________________________________________________
activation_1 (Activation)	 (None, 925)	       0	   dense_1[0][0]
____________________________________________________________________________________________________
dense_2 (Dense) 		 (None, 919)	       850994	   activation_1[0][0]
____________________________________________________________________________________________________
activation_2 (Activation)	 (None, 919)	       0	   dense_2[0][0]
====================================================================================================
Total params: 23,085,519
Trainable params: 23,085,519
Non-trainable params: 0
____________________________________________________________________________________________________
Saving models in json and yaml format to models/json/danq_4.json and  models/yaml/danq_4.yaml
Saving weights to models/weights/danq_4.hdf5 and epoch logs to models/csv/danq_4.csv
Saving models/json/danq_4.json to models/json/danq_4.json.old
Saving models/yaml/danq_4.yaml to models/yaml/danq_4.yaml.old
Retrieving train, validation, and test data

The date is 02/16/2017
The time is 02:42:40 AM

Loading weights from models/weights/danq_4.hdf5 if it exists
Saving models/weights/danq_4.hdf5 to models/weights/danq_4.hdf5.old
Saving models/csv/danq_4.csv to models/csv/danq_4.csv.old
Running at most 70 epochs
Train on 2200000 samples, validate on 8000 samples
Epoch 1/70
2199600/2200000 [============================>.] - ETA: 0ss--loss::0.08288--acc::0.9788Epoch 00000: val_loss improved from inf to 0.07321, saving model to models/weights/danq_4.hdf5
2200000/2200000 [==============================] - 2087s - loss: 0.0828 - acc: 0.9788 - val_loss: 0.0732 - val_acc: 0.9810
Epoch 2/70
2199600/2200000 [============================>.] - ETA: 0ss--loss::0.08133--acc::0.9788Epoch 00001: val_loss improved from 0.07321 to 0.07291, saving model to models/weights/danq_4.hdf5
2200000/2200000 [==============================] - 2082s - loss: 0.0813 - acc: 0.9788 - val_loss: 0.0729 - val_acc: 0.9810
Epoch 3/70
2199600/2200000 [============================>.] - ETA: 0ss--loss::0.08133--acc::0.9788Epoch 00002: val_loss improved from 0.07291 to 0.07233, saving model to models/weights/danq_4.hdf5
2200000/2200000 [==============================] - 2082s - loss: 0.0813 - acc: 0.9788 - val_loss: 0.0723 - val_acc: 0.9811
Epoch 4/70
2199600/2200000 [============================>.] - ETA: 0ss--loss::0.08122--acc::0.9788Epoch 00003: val_loss improved from 0.07233 to 0.07217, saving model to models/weights/danq_4.hdf5
2200000/2200000 [==============================] - 2087s - loss: 0.0812 - acc: 0.9788 - val_loss: 0.0722 - val_acc: 0.9811
Epoch 5/70
2199600/2200000 [============================>.] - ETA: 0ss--loss::0.08122--acc::0.9788Epoch 00004: val_loss improved from 0.07217 to 0.07204, saving model to models/weights/danq_4.hdf5
2200000/2200000 [==============================] - 2086s - loss: 0.0812 - acc: 0.9788 - val_loss: 0.0720 - val_acc: 0.9811
Epoch 6/70
2199600/2200000 [============================>.] - ETA: 0ss--loss::0.08122--acc::0.9788Epoch 00005: val_loss did not improve
2200000/2200000 [==============================] - 2086s - loss: 0.0812 - acc: 0.9788 - val_loss: 0.0722 - val_acc: 0.9810
Epoch 7/70
2199600/2200000 [============================>.] - ETA: 0ss--loss::0.08122--acc::0.9788Epoch 00006: val_loss did not improve
2200000/2200000 [==============================] - 2084s - loss: 0.0812 - acc: 0.9788 - val_loss: 0.0726 - val_acc: 0.9810
Epoch 8/70
2199600/2200000 [============================>.] - ETA: 0ss--loss::0.08133--acc::0.9788Epoch 00007: val_loss improved from 0.07204 to 0.07203, saving model to models/weights/danq_4.hdf5
2200000/2200000 [==============================] - 2084s - loss: 0.0813 - acc: 0.9788 - val_loss: 0.0720 - val_acc: 0.9811
Epoch 9/70
2199600/2200000 [============================>.] - ETA: 0ss--loss::0.08133--acc::0.9788Epoch 00008: val_loss did not improve
2200000/2200000 [==============================] - 2086s - loss: 0.0813 - acc: 0.9788 - val_loss: 0.0720 - val_acc: 0.9810
Epoch 10/70
2199600/2200000 [============================>.] - ETA: 0ss--loss::0.08133--acc::0.9788Epoch 00009: val_loss did not improve
2200000/2200000 [==============================] - 2083s - loss: 0.0813 - acc: 0.9788 - val_loss: 0.0726 - val_acc: 0.9810
Epoch 11/70
2199600/2200000 [============================>.] - ETA: 0ss--loss::0.08133--acc::0.9788Epoch 00010: val_loss did not improve
2200000/2200000 [==============================] - 2085s - loss: 0.0813 - acc: 0.9788 - val_loss: 0.0727 - val_acc: 0.9810
Epoch 12/70
2199600/2200000 [============================>.] - ETA: 0ss--loss::0.08133--acc::0.9788Epoch 00011: val_loss did not improve
2200000/2200000 [==============================] - 2084s - loss: 0.0813 - acc: 0.9788 - val_loss: 0.0722 - val_acc: 0.9810
Epoch 13/70
2199600/2200000 [============================>.] - ETA: 0ss--loss::0.08144--acc::0.9788Epoch 00012: val_loss did not improve
2200000/2200000 [==============================] - 2084s - loss: 0.0814 - acc: 0.9788 - val_loss: 0.0723 - val_acc: 0.9810
Epoch 14/70
2199600/2200000 [============================>.] - ETA: 0ss--loss::0.08144--acc::0.9788Epoch 00013: val_loss did not improve
2200000/2200000 [==============================] - 2083s - loss: 0.0814 - acc: 0.9788 - val_loss: 0.0729 - val_acc: 0.9810
Epoch 00013: early stopping
455008/455024 [============================>.] - ETA: 0s
[0.07721499704601828, 0.979817180782939]

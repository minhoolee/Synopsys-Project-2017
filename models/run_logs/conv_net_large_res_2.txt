2017-03-17 22:18:32,826 src.models.keras_model_utils INFO     Creating model from conv_net()
2017-03-17 22:18:32,826 src.models.create_models INFO     Building the model
2017-03-17 22:18:34,053 src.models.create_models INFO     Compiling model
____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to
====================================================================================================
input (InputLayer)               (None, 1000, 4)       0
____________________________________________________________________________________________________
block1_conv1 (Convolution1D)     (None, 998, 64)       832         input[0][0]
____________________________________________________________________________________________________
block1_prelu1 (PReLU)            (None, 998, 64)       63872       block1_conv1[0][0]
____________________________________________________________________________________________________
block1_batchnorm1 (BatchNormaliz (None, 998, 64)       256         block1_prelu1[0][0]
____________________________________________________________________________________________________
block1_pool1 (MaxPooling1D)      (None, 499, 64)       0           block1_batchnorm1[0][0]
____________________________________________________________________________________________________
block1_conv2 (Convolution1D)     (None, 497, 64)       12352       block1_pool1[0][0]
____________________________________________________________________________________________________
block1_prelu2 (PReLU)            (None, 497, 64)       31808       block1_conv2[0][0]
____________________________________________________________________________________________________
block1_batchnorm2 (BatchNormaliz (None, 497, 64)       256         block1_prelu2[0][0]
____________________________________________________________________________________________________
block1_pool2 (MaxPooling1D)      (None, 248, 64)       0           block1_batchnorm2[0][0]
____________________________________________________________________________________________________
block2_conv1 (Convolution1D)     (None, 246, 128)      24704       block1_pool2[0][0]
____________________________________________________________________________________________________
block2_prelu1 (PReLU)            (None, 246, 128)      31488       block2_conv1[0][0]
____________________________________________________________________________________________________
block2_batchnorm1 (BatchNormaliz (None, 246, 128)      512         block2_prelu1[0][0]
____________________________________________________________________________________________________
block2_conv2 (Convolution1D)     (None, 244, 128)      49280       block2_batchnorm1[0][0]
____________________________________________________________________________________________________
block2_prelu2 (PReLU)            (None, 244, 128)      31232       block2_conv2[0][0]
____________________________________________________________________________________________________
block2_batchnorm2 (BatchNormaliz (None, 244, 128)      512         block2_prelu2[0][0]
____________________________________________________________________________________________________
block2_dropout1 (Dropout)        (None, 244, 128)      0           block2_batchnorm2[0][0]
____________________________________________________________________________________________________
block2_pool1 (MaxPooling1D)      (None, 122, 128)      0           block2_dropout1[0][0]
____________________________________________________________________________________________________
block3_conv1 (Convolution1D)     (None, 120, 256)      98560       block2_pool1[0][0]
____________________________________________________________________________________________________
block3_prelu1 (PReLU)            (None, 120, 256)      30720       block3_conv1[0][0]
____________________________________________________________________________________________________
block3_batchnorm1 (BatchNormaliz (None, 120, 256)      1024        block3_prelu1[0][0]
____________________________________________________________________________________________________
block3_dropout1 (Dropout)        (None, 120, 256)      0           block3_batchnorm1[0][0]
____________________________________________________________________________________________________
block3_conv2 (Convolution1D)     (None, 118, 256)      196864      block3_dropout1[0][0]
____________________________________________________________________________________________________
block3_prelu2 (PReLU)            (None, 118, 256)      30208       block3_conv2[0][0]
____________________________________________________________________________________________________
block3_batchnorm2 (BatchNormaliz (None, 118, 256)      1024        block3_prelu2[0][0]
____________________________________________________________________________________________________
block3_dropout2 (Dropout)        (None, 118, 256)      0           block3_batchnorm2[0][0]
____________________________________________________________________________________________________
block3_pool1 (MaxPooling1D)      (None, 59, 256)       0           block3_dropout2[0][0]
____________________________________________________________________________________________________
wide_res_batchnorm1_block1_0_1 ( (None, 59, 256)       1024        block3_pool1[0][0]
____________________________________________________________________________________________________
wide_res_relu1_block1_0_1 (Activ (None, 59, 256)       0           wide_res_batchnorm1_block1_0_1[0]
____________________________________________________________________________________________________
wide_res_conv1_block1_0_1 (Convo (None, 59, 64)        49152       wide_res_relu1_block1_0_1[0][0]
____________________________________________________________________________________________________
wide_res_batchnorm1_block1_1_1 ( (None, 59, 64)        256         wide_res_conv1_block1_0_1[0][0]
____________________________________________________________________________________________________
wide_res_relu1_block1_1_1 (Activ (None, 59, 64)        0           wide_res_batchnorm1_block1_1_1[0]
____________________________________________________________________________________________________
wide_res_conv1_block1_1_1 (Convo (None, 59, 64)        12288       wide_res_relu1_block1_1_1[0][0]
____________________________________________________________________________________________________
wide_res_shortcut_block1_1_1 (Co (None, 59, 64)        16384       wide_res_relu1_block1_0_1[0][0]
____________________________________________________________________________________________________
merge_1 (Merge)                  (None, 59, 64)        0           wide_res_conv1_block1_1_1[0][0]
                                                                   wide_res_shortcut_block1_1_1[0][0
____________________________________________________________________________________________________
wide_res_batchnorm1_block1_0_2 ( (None, 59, 64)        256         merge_1[0][0]
____________________________________________________________________________________________________
wide_res_relu1_block1_0_2 (Activ (None, 59, 64)        0           wide_res_batchnorm1_block1_0_2[0]
____________________________________________________________________________________________________
wide_res_conv1_block1_0_2 (Convo (None, 59, 64)        12288       wide_res_relu1_block1_0_2[0][0]
____________________________________________________________________________________________________
wide_res_batchnorm1_block1_1_2 ( (None, 59, 64)        256         wide_res_conv1_block1_0_2[0][0]
____________________________________________________________________________________________________
wide_res_relu1_block1_1_2 (Activ (None, 59, 64)        0           wide_res_batchnorm1_block1_1_2[0]
____________________________________________________________________________________________________
wide_res_conv1_block1_1_2 (Convo (None, 59, 64)        12288       wide_res_relu1_block1_1_2[0][0]
____________________________________________________________________________________________________
merge_2 (Merge)                  (None, 59, 64)        0           wide_res_conv1_block1_1_2[0][0]
                                                                   merge_1[0][0]
____________________________________________________________________________________________________
wide_res_batchnorm1_block2_0_1 ( (None, 59, 64)        256         merge_2[0][0]
____________________________________________________________________________________________________
wide_res_relu1_block2_0_1 (Activ (None, 59, 64)        0           wide_res_batchnorm1_block2_0_1[0]
____________________________________________________________________________________________________
wide_res_conv1_block2_0_1 (Convo (None, 30, 128)       24576       wide_res_relu1_block2_0_1[0][0]
____________________________________________________________________________________________________
wide_res_batchnorm1_block2_1_1 ( (None, 30, 128)       512         wide_res_conv1_block2_0_1[0][0]
____________________________________________________________________________________________________
wide_res_relu1_block2_1_1 (Activ (None, 30, 128)       0           wide_res_batchnorm1_block2_1_1[0]
____________________________________________________________________________________________________
wide_res_conv1_block2_1_1 (Convo (None, 30, 128)       49152       wide_res_relu1_block2_1_1[0][0]
____________________________________________________________________________________________________
wide_res_shortcut_block2_1_1 (Co (None, 30, 128)       8192        wide_res_relu1_block2_0_1[0][0]
____________________________________________________________________________________________________
merge_3 (Merge)                  (None, 30, 128)       0           wide_res_conv1_block2_1_1[0][0]
                                                                   wide_res_shortcut_block2_1_1[0][0
____________________________________________________________________________________________________
wide_res_batchnorm1_block2_0_2 ( (None, 30, 128)       512         merge_3[0][0]
____________________________________________________________________________________________________
wide_res_relu1_block2_0_2 (Activ (None, 30, 128)       0           wide_res_batchnorm1_block2_0_2[0]
____________________________________________________________________________________________________
wide_res_conv1_block2_0_2 (Convo (None, 30, 128)       49152       wide_res_relu1_block2_0_2[0][0]
____________________________________________________________________________________________________
wide_res_batchnorm1_block2_1_2 ( (None, 30, 128)       512         wide_res_conv1_block2_0_2[0][0]
____________________________________________________________________________________________________
wide_res_relu1_block2_1_2 (Activ (None, 30, 128)       0           wide_res_batchnorm1_block2_1_2[0]
____________________________________________________________________________________________________
wide_res_conv1_block2_1_2 (Convo (None, 30, 128)       49152       wide_res_relu1_block2_1_2[0][0]
____________________________________________________________________________________________________
merge_4 (Merge)                  (None, 30, 128)       0           wide_res_conv1_block2_1_2[0][0]
                                                                   merge_3[0][0]
____________________________________________________________________________________________________
wide_res_net_batchnorm1 (BatchNo (None, 30, 128)       512         merge_4[0][0]
____________________________________________________________________________________________________
wide_res_net_relu1 (Activation)  (None, 30, 128)       0           wide_res_net_batchnorm1[0][0]
____________________________________________________________________________________________________
dropout1 (Dropout)               (None, 30, 128)       0           wide_res_net_relu1[0][0]
____________________________________________________________________________________________________
dropout1_pool1 (MaxPooling1D)    (None, 15, 128)       0           dropout1[0][0]
____________________________________________________________________________________________________
bidirectional_1 (Bidirectional)  (None, 15, 512)       591360      dropout1_pool1[0][0]
____________________________________________________________________________________________________
gru_dropout1 (Dropout)           (None, 15, 512)       0           bidirectional_1[0][0]
____________________________________________________________________________________________________
flatten (Flatten)                (None, 7680)          0           gru_dropout1[0][0]
____________________________________________________________________________________________________
fc1 (Dense)                      (None, 1024)          7865344     flatten[0][0]
____________________________________________________________________________________________________
fc1_prelu (PReLU)                (None, 1024)          1024        fc1[0][0]
____________________________________________________________________________________________________
fc1_batchnorm (BatchNormalizatio (None, 1024)          4096        fc1_prelu[0][0]
____________________________________________________________________________________________________
fc1_dropout (Dropout)            (None, 1024)          0           fc1_batchnorm[0][0]
____________________________________________________________________________________________________
fc2 (Dense)                      (None, 919)           941975      fc1_dropout[0][0]
____________________________________________________________________________________________________
fc2_sigmoid (Activation)         (None, 919)           0           fc2[0][0]
====================================================================================================
Total params: 10,296,023
Trainable params: 10,290,135
Non-trainable params: 5,888
____________________________________________________________________________________________________
2017-03-17 22:18:36,572 __main__     INFO     Retrieving training data...
2017-03-17 22:20:04,725 __main__     INFO     Retrieving validation data...
2017-03-17 22:20:04,819 __main__     INFO     Saving model to file system...
2017-03-17 22:20:04,819 src.models.keras_model_utils INFO     Saving models/json/conv_net_large_res_2.json to models/json/conv_net_large_res_2.json.old
2017-03-17 22:20:04,819 src.models.keras_model_utils INFO     Saving model to models/json/conv_net_large_res_2.json
2017-03-17 22:20:04,823 src.models.keras_model_utils INFO     Saving models/yaml/conv_net_large_res_2.yaml to models/yaml/conv_net_large_res_2.yaml.old
2017-03-17 22:20:04,824 src.models.keras_model_utils INFO     Saving model to models/yaml/conv_net_large_res_2.yaml
2017-03-17 22:20:04,881 __main__     INFO     Loading model weights...
2017-03-17 22:20:04,881 __main__     INFO     The date is 03/17/2017
2017-03-17 22:20:04,881 __main__     INFO     The time is 10:20:04 PM

2017-03-17 22:20:04,881 __main__     INFO     Training model...
2017-03-17 22:20:04,881 src.models.keras_model_utils INFO     Saving models/csv/conv_net_large_res_2.csv to models/csv/conv_net_large_res_2.csv.old
2017-03-17 22:20:04,882 src.models.keras_model_utils INFO     Running at most 70 epochs
2017-03-17 22:20:04,882 src.models.keras_model_utils INFO     Saving weights to models/weights/conv_net_large_res_2.hdf5
2017-03-17 22:20:04,882 src.models.keras_model_utils INFO     Saving epoch logs to models/csv/conv_net_large_res_2.csv
Train on 4400000 samples, validate on 8000 samples
Epoch 1/70
4399600/4400000 [============================>.] - ETA: 0s - loss: 0.0776 - acc: 0.9780Epoch 00000: val_loss improved from inf to 0.06485, saving model to models/weights/conv_net_large_res_2.hdf5
4400000/4400000 [==============================] - 6008s - loss: 0.0776 - acc: 0.9780 - val_loss: 0.0648 - val_acc: 0.9822
Epoch 2/70
4399600/4400000 [============================>.] - ETA: 0s - loss: 0.0677 - acc: 0.9801Epoch 00001: val_loss improved from 0.06485 to 0.06006, saving model to models/weights/conv_net_large_res_2.hdf5
4400000/4400000 [==============================] - 5990s - loss: 0.0677 - acc: 0.9801 - val_loss: 0.0601 - val_acc: 0.9823
Epoch 3/70
4399600/4400000 [============================>.] - ETA: 0s - loss: 0.0659 - acc: 0.9803Epoch 00002: val_loss improved from 0.06006 to 0.05900, saving model to models/weights/conv_net_large_res_2.hdf5
4400000/4400000 [==============================] - 5990s - loss: 0.0659 - acc: 0.9803 - val_loss: 0.0590 - val_acc: 0.9825
Epoch 4/70
4399600/4400000 [============================>.] - ETA: 0s - loss: 0.0649 - acc: 0.9805Epoch 00003: val_loss improved from 0.05900 to 0.05850, saving model to models/weights/conv_net_large_res_2.hdf5
4400000/4400000 [==============================] - 5991s - loss: 0.0649 - acc: 0.9805 - val_loss: 0.0585 - val_acc: 0.9825
Epoch 5/70
4399600/4400000 [============================>.] - ETA: 0s - loss: 0.0643 - acc: 0.9806Epoch 00004: val_loss did not improve
4400000/4400000 [==============================] - 5989s - loss: 0.0643 - acc: 0.9806 - val_loss: 0.0589 - val_acc: 0.9826
Epoch 6/70
4399600/4400000 [============================>.] - ETA: 0s - loss: 0.0638 - acc: 0.9807Epoch 00005: val_loss did not improve
4400000/4400000 [==============================] - 5994s - loss: 0.0638 - acc: 0.9807 - val_loss: 0.0589 - val_acc: 0.9826
Epoch 7/70
4399600/4400000 [============================>.] - ETA: 0s - loss: 0.0634 - acc: 0.9807Epoch 00006: val_loss did not improve
4400000/4400000 [==============================] - 7088s - loss: 0.0634 - acc: 0.9807 - val_loss: 0.0588 - val_acc: 0.9826
Epoch 8/70
4399600/4400000 [============================>.] - ETA: 0s - loss: 0.0631 - acc: 0.9808Epoch 00007: val_loss improved from 0.05850 to 0.05797, saving model to models/weights/conv_net_large_res_2.hdf5
4400000/4400000 [==============================] - 7060s - loss: 0.0631 - acc: 0.9808 - val_loss: 0.0580 - val_acc: 0.9827
Epoch 9/70
4399600/4400000 [============================>.] - ETA: 0s - loss: 0.0629 - acc: 0.9808Epoch 00008: val_loss did not improve
4400000/4400000 [==============================] - 8402s - loss: 0.0629 - acc: 0.9808 - val_loss: 0.0616 - val_acc: 0.9827
Epoch 10/70
4399600/4400000 [============================>.] - ETA: 0s - loss: 0.0627 - acc: 0.9808Epoch 00009: val_loss did not improve
4400000/4400000 [==============================] - 6773s - loss: 0.0627 - acc: 0.9808 - val_loss: 0.0591 - val_acc: 0.9826
Epoch 11/70
4399600/4400000 [============================>.] - ETA: 0s - loss: 0.0625 - acc: 0.9809Epoch 00010: val_loss did not improve
4400000/4400000 [==============================] - 8856s - loss: 0.0625 - acc: 0.9809 - val_loss: 0.0620 - val_acc: 0.9826

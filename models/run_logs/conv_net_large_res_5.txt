2017-03-23 02:01:36,302 src.models.keras_model_utils INFO     Creating model from conv_net()...
2017-03-23 02:01:36,302 src.models.create_models INFO     Building the model
2017-03-23 02:01:37,356 src.models.create_models INFO     Compiling model
____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to
====================================================================================================
input (InputLayer)               (None, 1000, 4)       0
____________________________________________________________________________________________________
block1_conv1 (Convolution1D)     (None, 998, 64)       832         input[0][0]
____________________________________________________________________________________________________
activation_1 (Activation)        (None, 998, 64)       0           block1_conv1[0][0]
____________________________________________________________________________________________________
batchnormalization_1 (BatchNorma (None, 998, 64)       256         activation_1[0][0]
____________________________________________________________________________________________________
block1_pool1 (MaxPooling1D)      (None, 499, 64)       0           batchnormalization_1[0][0]
____________________________________________________________________________________________________
block1_conv2 (Convolution1D)     (None, 497, 64)       12352       block1_pool1[0][0]
____________________________________________________________________________________________________
activation_2 (Activation)        (None, 497, 64)       0           block1_conv2[0][0]
____________________________________________________________________________________________________
batchnormalization_2 (BatchNorma (None, 497, 64)       256         activation_2[0][0]
____________________________________________________________________________________________________
block1_pool2 (MaxPooling1D)      (None, 248, 64)       0           batchnormalization_2[0][0]
____________________________________________________________________________________________________
block2_conv1 (Convolution1D)     (None, 246, 128)      24704       block1_pool2[0][0]
____________________________________________________________________________________________________
activation_3 (Activation)        (None, 246, 128)      0           block2_conv1[0][0]
____________________________________________________________________________________________________
batchnormalization_3 (BatchNorma (None, 246, 128)      512         activation_3[0][0]
____________________________________________________________________________________________________
block2_conv2 (Convolution1D)     (None, 244, 128)      49280       batchnormalization_3[0][0]
____________________________________________________________________________________________________
activation_4 (Activation)        (None, 244, 128)      0           block2_conv2[0][0]
____________________________________________________________________________________________________
batchnormalization_4 (BatchNorma (None, 244, 128)      512         activation_4[0][0]
____________________________________________________________________________________________________
block2_dropout1 (Dropout)        (None, 244, 128)      0           batchnormalization_4[0][0]
____________________________________________________________________________________________________
block2_pool1 (MaxPooling1D)      (None, 122, 128)      0           block2_dropout1[0][0]
____________________________________________________________________________________________________
block3_conv1 (Convolution1D)     (None, 120, 256)      98560       block2_pool1[0][0]
____________________________________________________________________________________________________
activation_5 (Activation)        (None, 120, 256)      0           block3_conv1[0][0]
____________________________________________________________________________________________________
batchnormalization_5 (BatchNorma (None, 120, 256)      1024        activation_5[0][0]
____________________________________________________________________________________________________
block3_dropout1 (Dropout)        (None, 120, 256)      0           batchnormalization_5[0][0]
____________________________________________________________________________________________________
block3_pool1 (MaxPooling1D)      (None, 60, 256)       0           block3_dropout1[0][0]
____________________________________________________________________________________________________
block3_conv2 (Convolution1D)     (None, 58, 256)       196864      block3_pool1[0][0]
____________________________________________________________________________________________________
activation_6 (Activation)        (None, 58, 256)       0           block3_conv2[0][0]
____________________________________________________________________________________________________
batchnormalization_6 (BatchNorma (None, 58, 256)       1024        activation_6[0][0]
____________________________________________________________________________________________________
block3_dropout2 (Dropout)        (None, 58, 256)       0           batchnormalization_6[0][0]
____________________________________________________________________________________________________
block3_pool2 (MaxPooling1D)      (None, 29, 256)       0           block3_dropout2[0][0]
____________________________________________________________________________________________________
block4_conv1 (Convolution1D)     (None, 27, 512)       393728      block3_pool2[0][0]
____________________________________________________________________________________________________
activation_7 (Activation)        (None, 27, 512)       0           block4_conv1[0][0]
____________________________________________________________________________________________________
batchnormalization_7 (BatchNorma (None, 27, 512)       2048        activation_7[0][0]
____________________________________________________________________________________________________
block4_dropout1 (Dropout)        (None, 27, 512)       0           batchnormalization_7[0][0]
____________________________________________________________________________________________________
block4_pool1 (MaxPooling1D)      (None, 13, 512)       0           block4_dropout1[0][0]
____________________________________________________________________________________________________
block4_conv2 (Convolution1D)     (None, 11, 512)       786944      block4_pool1[0][0]
____________________________________________________________________________________________________
activation_8 (Activation)        (None, 11, 512)       0           block4_conv2[0][0]
____________________________________________________________________________________________________
batchnormalization_8 (BatchNorma (None, 11, 512)       2048        activation_8[0][0]
____________________________________________________________________________________________________
dropout1 (Dropout)               (None, 11, 512)       0           batchnormalization_8[0][0]
____________________________________________________________________________________________________
dropout1_pool1 (MaxPooling1D)    (None, 5, 512)        0           dropout1[0][0]
____________________________________________________________________________________________________
bidirectional_1 (Bidirectional)  (None, 5, 512)        1181184     dropout1_pool1[0][0]
____________________________________________________________________________________________________
gru_dropout1 (Dropout)           (None, 5, 512)        0           bidirectional_1[0][0]
____________________________________________________________________________________________________
flatten (Flatten)                (None, 2560)          0           gru_dropout1[0][0]
____________________________________________________________________________________________________
fc1 (Dense)                      (None, 1024)          2622464     flatten[0][0]
____________________________________________________________________________________________________
activation_9 (Activation)        (None, 1024)          0           fc1[0][0]
____________________________________________________________________________________________________
batchnormalization_9 (BatchNorma (None, 1024)          4096        activation_9[0][0]
____________________________________________________________________________________________________
fc1_dropout (Dropout)            (None, 1024)          0           batchnormalization_9[0][0]
____________________________________________________________________________________________________
fc2 (Dense)                      (None, 919)           941975      fc1_dropout[0][0]
____________________________________________________________________________________________________
fc2_sigmoid (Activation)         (None, 919)           0           fc2[0][0]
====================================================================================================
Total params: 6,320,663
Trainable params: 6,314,775
Non-trainable params: 5,888
____________________________________________________________________________________________________
2017-03-23 02:01:37,392 __main__     INFO

2017-03-23 02:01:37,392 __main__     INFO     Loading model weights...
2017-03-23 02:01:37,392 src.models.keras_model_utils INFO     Loading weights from models/weights/conv_net_large_res_5.hdf5...
2017-03-23 02:01:38,137 __main__     INFO     Retrieving training data...
2017-03-23 02:02:57,287 __main__     INFO     Retrieving validation data...
2017-03-23 02:02:57,384 __main__     INFO     The date is 03/23/2017
2017-03-23 02:02:57,384 __main__     INFO     The time is 02:02:57 AM
2017-03-23 02:02:57,384 __main__     INFO

2017-03-23 02:02:57,384 __main__     INFO     Training model...
2017-03-23 02:02:57,384 src.models.keras_model_utils INFO     Saving models/csv/conv_net_large_res_5.csv to models/csv/conv_net_large_res_5.csv.old...
2017-03-23 02:02:57,385 src.models.keras_model_utils INFO     Running at most 70 epochs
2017-03-23 02:02:57,385 src.models.keras_model_utils INFO     Saving weights to models/weights/conv_net_large_res_5.hdf5...
2017-03-23 02:02:57,385 src.models.keras_model_utils INFO     Saving epoch logs to models/csv/conv_net_large_res_5.csv...
Train on 4400000 samples, validate on 8000 samples
Epoch 1/70
    400/4400000 [..............................] - ETA: 45782s - loss: 6.8641 - acc: 0.4802I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 2531 get requests, put_count=2341 evicted_count=1000 eviction_rate=0.427168 and unsatisfied allocation rate=0.50968
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 100 to 110
   8400/4400000 [..............................] - ETA: 4531s - loss: 6.0144 - acc: 0.5209I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 3788 get requests, put_count=3496 evicted_count=1000 eviction_rate=0.286041 and unsatisfied allocation rate=0.347149
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 256 to 281
  34400/4400000 [..............................] - ETA: 2966s - loss: 3.9392 - acc: 0.6458I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 31675 get requests, put_count=31681 evicted_count=1000 eviction_rate=0.0315647 and unsatisfied allocation rate=0.0332439
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 655 to 720
4399600/4400000 [============================>.] - ETA: 0s - loss: 0.1310 - acc: 0.9748Epoch 00000: saving model to models/weights/conv_net_large_res_5.00-0.070.hdf5
4400000/4400000 [==============================] - 2509s - loss: 0.1310 - acc: 0.9748 - val_loss: 0.0704 - val_acc: 0.9810
Epoch 2/70
4399600/4400000 [============================>.] - ETA: 0s - loss: 0.0719 - acc: 0.9795Epoch 00001: saving model to models/weights/conv_net_large_res_5.01-0.063.hdf5
4400000/4400000 [==============================] - 2513s - loss: 0.0719 - acc: 0.9795 - val_loss: 0.0635 - val_acc: 0.9819
Epoch 3/70
4399600/4400000 [============================>.] - ETA: 0s - loss: 0.0689 - acc: 0.9799Epoch 00002: saving model to models/weights/conv_net_large_res_5.02-0.061.hdf5
4400000/4400000 [==============================] - 2534s - loss: 0.0689 - acc: 0.9799 - val_loss: 0.0610 - val_acc: 0.9822
Epoch 4/70
4399600/4400000 [============================>.] - ETA: 0s - loss: 0.0673 - acc: 0.9801Epoch 00003: saving model to models/weights/conv_net_large_res_5.03-0.059.hdf5
4400000/4400000 [==============================] - 2537s - loss: 0.0674 - acc: 0.9801 - val_loss: 0.0590 - val_acc: 0.9825
Epoch 5/70
4399600/4400000 [============================>.] - ETA: 0s - loss: 0.0663 - acc: 0.9802Epoch 00004: saving model to models/weights/conv_net_large_res_5.04-0.059.hdf5
4400000/4400000 [==============================] - 2536s - loss: 0.0663 - acc: 0.9802 - val_loss: 0.0589 - val_acc: 0.9825
Epoch 6/70
4399600/4400000 [============================>.] - ETA: 0s - loss: 0.0657 - acc: 0.9803Epoch 00005: saving model to models/weights/conv_net_large_res_5.05-0.058.hdf5
4400000/4400000 [==============================] - 2536s - loss: 0.0657 - acc: 0.9803 - val_loss: 0.0584 - val_acc: 0.9827
Epoch 7/70
4399600/4400000 [============================>.] - ETA: 0s - loss: 0.0651 - acc: 0.9804Epoch 00006: saving model to models/weights/conv_net_large_res_5.06-0.058.hdf5
4400000/4400000 [==============================] - 2537s - loss: 0.0651 - acc: 0.9804 - val_loss: 0.0580 - val_acc: 0.9827
Epoch 8/70
4399600/4400000 [============================>.] - ETA: 0s - loss: 0.0645 - acc: 0.9804Epoch 00007: saving model to models/weights/conv_net_large_res_5.07-0.057.hdf5
4400000/4400000 [==============================] - 2537s - loss: 0.0645 - acc: 0.9804 - val_loss: 0.0571 - val_acc: 0.9828
Epoch 9/70
4399600/4400000 [============================>.] - ETA: 0s - loss: 0.0641 - acc: 0.9805Epoch 00008: saving model to models/weights/conv_net_large_res_5.08-0.057.hdf5
4400000/4400000 [==============================] - 2534s - loss: 0.0641 - acc: 0.9805 - val_loss: 0.0565 - val_acc: 0.9828
Epoch 10/70
4399600/4400000 [============================>.] - ETA: 0s - loss: 0.0638 - acc: 0.9805Epoch 00009: saving model to models/weights/conv_net_large_res_5.09-0.057.hdf5
4400000/4400000 [==============================] - 2517s - loss: 0.0638 - acc: 0.9805 - val_loss: 0.0569 - val_acc: 0.9828
Epoch 11/70
4399600/4400000 [============================>.] - ETA: 0s - loss: 0.0635 - acc: 0.9805Epoch 00010: saving model to models/weights/conv_net_large_res_5.10-0.056.hdf5
4400000/4400000 [==============================] - 2467s - loss: 0.0635 - acc: 0.9805 - val_loss: 0.0563 - val_acc: 0.9829
Epoch 12/70
4399600/4400000 [============================>.] - ETA: 0s - loss: 0.0633 - acc: 0.9806Epoch 00011: saving model to models/weights/conv_net_large_res_5.11-0.055.hdf5
4400000/4400000 [==============================] - 2443s - loss: 0.0633 - acc: 0.9806 - val_loss: 0.0554 - val_acc: 0.9831
Epoch 13/70
4399600/4400000 [============================>.] - ETA: 0s - loss: 0.0631 - acc: 0.9806Epoch 00012: saving model to models/weights/conv_net_large_res_5.12-0.056.hdf5
4400000/4400000 [==============================] - 2410s - loss: 0.0631 - acc: 0.9806 - val_loss: 0.0560 - val_acc: 0.9830
Epoch 14/70
4399600/4400000 [============================>.] - ETA: 0s - loss: 0.0629 - acc: 0.9806Epoch 00013: saving model to models/weights/conv_net_large_res_5.13-0.056.hdf5
4400000/4400000 [==============================] - 2333s - loss: 0.0629 - acc: 0.9806 - val_loss: 0.0555 - val_acc: 0.9830
Epoch 15/70
4399600/4400000 [============================>.] - ETA: 0s - loss: 0.0628 - acc: 0.9806Epoch 00014: saving model to models/weights/conv_net_large_res_5.14-0.055.hdf5
4400000/4400000 [==============================] - 2309s - loss: 0.0628 - acc: 0.9806 - val_loss: 0.0553 - val_acc: 0.9831
Epoch 16/70
4399600/4400000 [============================>.] - ETA: 0s - loss: 0.0626 - acc: 0.9807Epoch 00015: saving model to models/weights/conv_net_large_res_5.15-0.055.hdf5
4400000/4400000 [==============================] - 2289s - loss: 0.0626 - acc: 0.9807 - val_loss: 0.0553 - val_acc: 0.9831
Epoch 17/70
4399600/4400000 [============================>.] - ETA: 0s - loss: 0.0625 - acc: 0.9807Epoch 00016: saving model to models/weights/conv_net_large_res_5.16-0.055.hdf5
4400000/4400000 [==============================] - 2305s - loss: 0.0625 - acc: 0.9807 - val_loss: 0.0553 - val_acc: 0.9831
Epoch 18/70
4399600/4400000 [============================>.] - ETA: 0s - loss: 0.0624 - acc: 0.9807Epoch 00017: saving model to models/weights/conv_net_large_res_5.17-0.055.hdf5
4400000/4400000 [==============================] - 2289s - loss: 0.0624 - acc: 0.9807 - val_loss: 0.0550 - val_acc: 0.9831
Epoch 19/70
4399600/4400000 [============================>.] - ETA: 0s - loss: 0.0622 - acc: 0.9807Epoch 00018: saving model to models/weights/conv_net_large_res_5.18-0.055.hdf5
4400000/4400000 [==============================] - 2293s - loss: 0.0622 - acc: 0.9807 - val_loss: 0.0549 - val_acc: 0.9831
Epoch 20/70
4399600/4400000 [============================>.] - ETA: 0s - loss: 0.0621 - acc: 0.9808Epoch 00019: saving model to models/weights/conv_net_large_res_5.19-0.055.hdf5
4400000/4400000 [==============================] - 2279s - loss: 0.0621 - acc: 0.9808 - val_loss: 0.0553 - val_acc: 0.9830
Epoch 21/70
4399600/4400000 [============================>.] - ETA: 0s - loss: 0.0620 - acc: 0.9808Epoch 00020: saving model to models/weights/conv_net_large_res_5.20-0.055.hdf5
4400000/4400000 [==============================] - 2252s - loss: 0.0620 - acc: 0.9808 - val_loss: 0.0551 - val_acc: 0.9830
Epoch 22/70
4399600/4400000 [============================>.] - ETA: 0s - loss: 0.0619 - acc: 0.9808Epoch 00021: saving model to models/weights/conv_net_large_res_5.21-0.055.hdf5
4400000/4400000 [==============================] - 2252s - loss: 0.0619 - acc: 0.9808 - val_loss: 0.0550 - val_acc: 0.9832
Epoch 23/70
4399600/4400000 [============================>.] - ETA: 0s - loss: 0.0618 - acc: 0.9808Epoch 00022: saving model to models/weights/conv_net_large_res_5.22-0.055.hdf5
4400000/4400000 [==============================] - 2298s - loss: 0.0618 - acc: 0.9808 - val_loss: 0.0551 - val_acc: 0.9832
Epoch 24/70
4399600/4400000 [============================>.] - ETA: 0s - loss: 0.0617 - acc: 0.9808Epoch 00023: saving model to models/weights/conv_net_large_res_5.23-0.055.hdf5
4400000/4400000 [==============================] - 2297s - loss: 0.0617 - acc: 0.9808 - val_loss: 0.0553 - val_acc: 0.9830
Epoch 25/70
4399600/4400000 [============================>.] - ETA: 0s - loss: 0.0617 - acc: 0.9808Epoch 00024: saving model to models/weights/conv_net_large_res_5.24-0.055.hdf5
4400000/4400000 [==============================] - 2297s - loss: 0.0617 - acc: 0.9808 - val_loss: 0.0548 - val_acc: 0.9831
Epoch 26/70
4399600/4400000 [============================>.] - ETA: 0s - loss: 0.0616 - acc: 0.9808Epoch 00025: saving model to models/weights/conv_net_large_res_5.25-0.055.hdf5
4400000/4400000 [==============================] - 2292s - loss: 0.0616 - acc: 0.9808 - val_loss: 0.0548 - val_acc: 0.9831
Epoch 27/70
4399600/4400000 [============================>.] - ETA: 0s - loss: 0.0616 - acc: 0.9809Epoch 00026: saving model to models/weights/conv_net_large_res_5.26-0.054.hdf5
4400000/4400000 [==============================] - 2294s - loss: 0.0616 - acc: 0.9809 - val_loss: 0.0543 - val_acc: 0.9832
Epoch 28/70
4399600/4400000 [============================>.] - ETA: 0s - loss: 0.0615 - acc: 0.9809Epoch 00027: saving model to models/weights/conv_net_large_res_5.27-0.054.hdf5
4400000/4400000 [==============================] - 2294s - loss: 0.0615 - acc: 0.9809 - val_loss: 0.0543 - val_acc: 0.9832
Epoch 29/70
4399600/4400000 [============================>.] - ETA: 0s - loss: 0.0614 - acc: 0.9809Epoch 00028: saving model to models/weights/conv_net_large_res_5.28-0.054.hdf5
4400000/4400000 [==============================] - 2294s - loss: 0.0614 - acc: 0.9809 - val_loss: 0.0544 - val_acc: 0.9832
Epoch 30/70
4399600/4400000 [============================>.] - ETA: 0s - loss: 0.0614 - acc: 0.9809Epoch 00029: saving model to models/weights/conv_net_large_res_5.29-0.054.hdf5
4400000/4400000 [==============================] - 2295s - loss: 0.0614 - acc: 0.9809 - val_loss: 0.0542 - val_acc: 0.9833
Epoch 31/70
4399600/4400000 [============================>.] - ETA: 0s - loss: 0.0613 - acc: 0.9809Epoch 00030: saving model to models/weights/conv_net_large_res_5.30-0.055.hdf5
4400000/4400000 [==============================] - 2295s - loss: 0.0613 - acc: 0.9809 - val_loss: 0.0547 - val_acc: 0.9832
Epoch 32/70
4399600/4400000 [============================>.] - ETA: 0s - loss: 0.0612 - acc: 0.9809Epoch 00031: saving model to models/weights/conv_net_large_res_5.31-0.054.hdf5
4400000/4400000 [==============================] - 2294s - loss: 0.0612 - acc: 0.9809 - val_loss: 0.0545 - val_acc: 0.9832
Epoch 33/70
4399600/4400000 [============================>.] - ETA: 0s - loss: 0.0612 - acc: 0.9809Epoch 00032: saving model to models/weights/conv_net_large_res_5.32-0.054.hdf5
4400000/4400000 [==============================] - 2294s - loss: 0.0612 - acc: 0.9809 - val_loss: 0.0545 - val_acc: 0.9832
Epoch 34/70
4399600/4400000 [============================>.] - ETA: 0s - loss: 0.0612 - acc: 0.9809Epoch 00033: saving model to models/weights/conv_net_large_res_5.33-0.055.hdf5
4400000/4400000 [==============================] - 2295s - loss: 0.0612 - acc: 0.9809 - val_loss: 0.0548 - val_acc: 0.9831
Epoch 35/70
4399600/4400000 [============================>.] - ETA: 0s - loss: 0.0611 - acc: 0.9809Epoch 00034: saving model to models/weights/conv_net_large_res_5.34-0.054.hdf5
4400000/4400000 [==============================] - 2293s - loss: 0.0611 - acc: 0.9809 - val_loss: 0.0543 - val_acc: 0.9832
Epoch 36/70
4399600/4400000 [============================>.] - ETA: 0s - loss: 0.0611 - acc: 0.9809Epoch 00035: saving model to models/weights/conv_net_large_res_5.35-0.055.hdf5
4400000/4400000 [==============================] - 2293s - loss: 0.0611 - acc: 0.9809 - val_loss: 0.0546 - val_acc: 0.9832
Epoch 00035: early stopping
17-03-24 02:26:35,676 src.models.keras_model_utils INFO     [0.05962317824461167, 0.98160143734437222]

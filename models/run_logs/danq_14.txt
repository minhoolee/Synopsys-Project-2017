Creating model from DanQ()
Building the model
Compiling model
____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to
====================================================================================================
convolution1d_1 (Convolution1D)  (None, 975, 320)      33600       convolution1d_input_1[0][0]
____________________________________________________________________________________________________
maxpooling1d_1 (MaxPooling1D)    (None, 75, 320)       0           convolution1d_1[0][0]
____________________________________________________________________________________________________
bidirectional_1 (Bidirectional)  (None, 75, 640)       1640960     maxpooling1d_1[0][0]
____________________________________________________________________________________________________
flatten_1 (Flatten)              (None, 48000)         0           bidirectional_1[0][0]
____________________________________________________________________________________________________
dense_1 (Dense)                  (None, 925)           44400925    flatten_1[0][0]
____________________________________________________________________________________________________
activation_1 (Activation)        (None, 925)           0           dense_1[0][0]
____________________________________________________________________________________________________
dense_2 (Dense)                  (None, 919)           850994      activation_1[0][0]
____________________________________________________________________________________________________
activation_2 (Activation)        (None, 919)           0           dense_2[0][0]
====================================================================================================
Total params: 46,926,479
Trainable params: 46,926,479
Non-trainable params: 0
____________________________________________________________________________________________________
Saving models in json and yaml format to models/json/danq_14.json and  models/yaml/danq_14.yaml
Saving weights to models/weights/danq_14.hdf5 and epoch logs to models/csv/danq_14.csv
Retrieving train, validation, and test data

The date is 03/01/2017
The time is 08:24:46 AM

Running at most 70 epochs
Train on 2200000 samples, validate on 8000 samples
Epoch 1/70
2199600/2200000 [============================>.] - ETA: 0s - loss: 0.0768 - acc: 0.9791Epoch 00000: val_loss improved from inf to 0.06196, saving model to models/weights/danq_14.hdf5
2200000/2200000 [==============================] - 3834s - loss: 0.0768 - acc: 0.9791 - val_loss: 0.0620 - val_acc: 0.9821
Epoch 2/70
2199600/2200000 [============================>.] - ETA: 0s - loss: 0.0704 - acc: 0.9798Epoch 00001: val_loss improved from 0.06196 to 0.06059, saving model to models/weights/danq_14.hdf5
2200000/2200000 [==============================] - 3832s - loss: 0.0704 - acc: 0.9798 - val_loss: 0.0606 - val_acc: 0.9823
Epoch 3/70
2199600/2200000 [============================>.] - ETA: 0s - loss: 0.0691 - acc: 0.9800Epoch 00002: val_loss improved from 0.06059 to 0.06042, saving model to models/weights/danq_14.hdf5
2200000/2200000 [==============================] - 3834s - loss: 0.0691 - acc: 0.9800 - val_loss: 0.0604 - val_acc: 0.9823
Epoch 4/70
2199600/2200000 [============================>.] - ETA: 0s - loss: 0.0684 - acc: 0.9801Epoch 00003: val_loss improved from 0.06042 to 0.05994, saving model to models/weights/danq_14.hdf5
2200000/2200000 [==============================] - 3835s - loss: 0.0684 - acc: 0.9801 - val_loss: 0.0599 - val_acc: 0.9824
Epoch 5/70
2199600/2200000 [============================>.] - ETA: 0s - loss: 0.0680 - acc: 0.9801Epoch 00004: val_loss improved from 0.05994 to 0.05960, saving model to models/weights/danq_14.hdf5
2200000/2200000 [==============================] - 3835s - loss: 0.0680 - acc: 0.9801 - val_loss: 0.0596 - val_acc: 0.9825
Epoch 6/70
2199600/2200000 [============================>.] - ETA: 0s - loss: 0.0677 - acc: 0.9802Epoch 00005: val_loss improved from 0.05960 to 0.05928, saving model to models/weights/danq_14.hdf5
2200000/2200000 [==============================] - 3835s - loss: 0.0677 - acc: 0.9802 - val_loss: 0.0593 - val_acc: 0.9825
Epoch 7/70
2199600/2200000 [============================>.] - ETA: 0s - loss: 0.0674 - acc: 0.9802Epoch 00006: val_loss improved from 0.05928 to 0.05917, saving model to models/weights/danq_14.hdf5
2200000/2200000 [==============================] - 3837s - loss: 0.0674 - acc: 0.9802 - val_loss: 0.0592 - val_acc: 0.9826
Epoch 8/70
2199600/2200000 [============================>.] - ETA: 0s - loss: 0.0673 - acc: 0.9803Epoch 00007: val_loss did not improve
2200000/2200000 [==============================] - 3837s - loss: 0.0673 - acc: 0.9803 - val_loss: 0.0600 - val_acc: 0.9824
Epoch 9/70
2199600/2200000 [============================>.] - ETA: 0s - loss: 0.0671 - acc: 0.9803Epoch 00008: val_loss improved from 0.05917 to 0.05885, saving model to models/weights/danq_14.hdf5
2200000/2200000 [==============================] - 3838s - loss: 0.0671 - acc: 0.9803 - val_loss: 0.0588 - val_acc: 0.9826

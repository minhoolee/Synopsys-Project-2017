Retrieving train, validation, and test data
Building the model
Compiling model
Saving models in json and yaml format to models/json/conv_net_11.json and  models/yaml/conv_net_11.yaml
Saving weights to models/weights/conv_net_11.hdf5 and epoch logs to models/run_logs/conv_net_11.csv
Saving models/json/conv_net_11.json to models/json/conv_net_11.json.old
Saving models/yaml/conv_net_11.yaml to models/yaml/conv_net_11.yaml.old
____________________________________________________________________________________________________
Layer (type)			 Output Shape	       Param #	   Connected to
====================================================================================================
convolution1d_1 (Convolution1D)  (None, 998, 128)      1664	   convolution1d_input_1[0][0]
____________________________________________________________________________________________________
activation_1 (Activation)	 (None, 998, 128)      0	   convolution1d_1[0][0]
____________________________________________________________________________________________________
batchnormalization_1 (BatchNorma (None, 998, 128)      512	   activation_1[0][0]
____________________________________________________________________________________________________
maxpooling1d_1 (MaxPooling1D)	 (None, 499, 128)      0	   batchnormalization_1[0][0]
____________________________________________________________________________________________________
dropout_1 (Dropout)		 (None, 499, 128)      0	   maxpooling1d_1[0][0]
____________________________________________________________________________________________________
convolution1d_2 (Convolution1D)  (None, 497, 256)      98560	   dropout_1[0][0]
____________________________________________________________________________________________________
activation_2 (Activation)	 (None, 497, 256)      0	   convolution1d_2[0][0]
____________________________________________________________________________________________________
batchnormalization_2 (BatchNorma (None, 497, 256)      1024	   activation_2[0][0]
____________________________________________________________________________________________________
maxpooling1d_2 (MaxPooling1D)	 (None, 248, 256)      0	   batchnormalization_2[0][0]
____________________________________________________________________________________________________
dropout_2 (Dropout)		 (None, 248, 256)      0	   maxpooling1d_2[0][0]
____________________________________________________________________________________________________
convolution1d_3 (Convolution1D)  (None, 244, 512)      655872	   dropout_2[0][0]
____________________________________________________________________________________________________
activation_3 (Activation)	 (None, 244, 512)      0	   convolution1d_3[0][0]
____________________________________________________________________________________________________
batchnormalization_3 (BatchNorma (None, 244, 512)      2048	   activation_3[0][0]
____________________________________________________________________________________________________
maxpooling1d_3 (MaxPooling1D)	 (None, 122, 512)      0	   batchnormalization_3[0][0]
____________________________________________________________________________________________________
dropout_3 (Dropout)		 (None, 122, 512)      0	   maxpooling1d_3[0][0]
____________________________________________________________________________________________________
convolution1d_4 (Convolution1D)  (None, 118, 512)      1311232	   dropout_3[0][0]
____________________________________________________________________________________________________
activation_4 (Activation)	 (None, 118, 512)      0	   convolution1d_4[0][0]
____________________________________________________________________________________________________
maxpooling1d_4 (MaxPooling1D)	 (None, 59, 512)       0	   activation_4[0][0]
____________________________________________________________________________________________________
flatten_1 (Flatten)		 (None, 30208)	       0	   maxpooling1d_4[0][0]
____________________________________________________________________________________________________
dense_1 (Dense) 		 (None, 1024)	       30934016    flatten_1[0][0]
____________________________________________________________________________________________________
activation_5 (Activation)	 (None, 1024)	       0	   dense_1[0][0]
____________________________________________________________________________________________________
batchnormalization_4 (BatchNorma (None, 1024)	       4096	   activation_5[0][0]
____________________________________________________________________________________________________
dropout_4 (Dropout)		 (None, 1024)	       0	   batchnormalization_4[0][0]
____________________________________________________________________________________________________
dense_2 (Dense) 		 (None, 919)	       941975	   dropout_4[0][0]
____________________________________________________________________________________________________
activation_6 (Activation)	 (None, 919)	       0	   dense_2[0][0]
====================================================================================================
Total params: 33,950,999
Trainable params: 33,947,159
Non-trainable params: 3,840
____________________________________________________________________________________________________

The date is 02/10/2017
The time is 08:22:58 AM

Loading weights from models/weights/conv_net_11.hdf5 if it exists
Saving models/run_logs/conv_net_11.csv to models/run_logs/conv_net_11.csv.old
Running at most 70 epochs
Train on 2200000 samples, validate on 8000 samples
Epoch 1/70
2199600/2200000 [============================>.] - ETA: 0ss--loss::0.07712--acc::0.9776Epoch 00000: val_loss improved from inf to 0.05930, saving model to models/weights/conv_net_11.hdf5
2200000/2200000 [==============================] - 5282s - loss: 0.0771 - acc: 0.9776 - val_loss: 0.0593 - val_acc: 0.9826
Epoch 2/70
2199600/2200000 [============================>.] - ETA: 0ss--loss::0.06788--acc::0.9799Epoch 00001: val_loss improved from 0.05930 to 0.05800, saving model to models/weights/conv_net_11.hdf5
2200000/2200000 [==============================] - 5279s - loss: 0.0678 - acc: 0.9799 - val_loss: 0.0580 - val_acc: 0.9827
Epoch 3/70
2199600/2200000 [============================>.] - ETA: 0ss--loss::0.06599--acc::0.9802Epoch 00002: val_loss improved from 0.05800 to 0.05647, saving model to models/weights/conv_net_11.hdf5
2200000/2200000 [==============================] - 5279s - loss: 0.0659 - acc: 0.9802 - val_loss: 0.0565 - val_acc: 0.9829
Epoch 4/70
2199600/2200000 [============================>.] - ETA: 0ss--loss::0.06466--acc::0.9804Epoch 00003: val_loss did not improve
2200000/2200000 [==============================] - 5278s - loss: 0.0646 - acc: 0.9804 - val_loss: 0.0572 - val_acc: 0.9827
Epoch 5/70
2199600/2200000 [============================>.] - ETA: 0ss--loss::0.06366--acc::0.9806Epoch 00004: val_loss improved from 0.05647 to 0.05522, saving model to models/weights/conv_net_11.hdf5
2200000/2200000 [==============================] - 5278s - loss: 0.0636 - acc: 0.9806 - val_loss: 0.0552 - val_acc: 0.9830
Epoch 6/70
2199600/2200000 [============================>.] - ETA: 0ss--loss::0.06288--acc::0.9807Epoch 00005: val_loss did not improve
2200000/2200000 [==============================] - 5276s - loss: 0.0628 - acc: 0.9807 - val_loss: 0.0554 - val_acc: 0.9829
Epoch 7/70
2199600/2200000 [============================>.] - ETA: 0ss--loss::0.06222--acc::0.9808Epoch 00006: val_loss improved from 0.05522 to 0.05512, saving model to models/weights/conv_net_11.hdf5
2200000/2200000 [==============================] - 5279s - loss: 0.0622 - acc: 0.9808 - val_loss: 0.0551 - val_acc: 0.9829
Epoch 8/70
2199600/2200000 [============================>.] - ETA: 0ss--loss::0.06155--acc::0.9809Epoch 00007: val_loss improved from 0.05512 to 0.05510, saving model to models/weights/conv_net_11.hdf5
2200000/2200000 [==============================] - 5282s - loss: 0.0615 - acc: 0.9809 - val_loss: 0.0551 - val_acc: 0.9829
Epoch 9/70
2199600/2200000 [============================>.] - ETA: 0ss--loss::0.06111--acc::0.9810Epoch 00008: val_loss did not improve
2200000/2200000 [==============================] - 5281s - loss: 0.0611 - acc: 0.9810 - val_loss: 0.0552 - val_acc: 0.9829
Epoch 10/70
2199600/2200000 [============================>.] - ETA: 0ss--loss::0.06077--acc::0.9811Epoch 00009: val_loss improved from 0.05510 to 0.05484, saving model to models/weights/conv_net_11.hdf5
2200000/2200000 [==============================] - 5283s - loss: 0.0607 - acc: 0.9811 - val_loss: 0.0548 - val_acc: 0.9829
Epoch 11/70
2199600/2200000 [============================>.] - ETA: 0ss--loss::0.06033--acc::0.9812Epoch 00010: val_loss did not improve
2200000/2200000 [==============================] - 5285s - loss: 0.0603 - acc: 0.9812 - val_loss: 0.0552 - val_acc: 0.9828
Epoch 12/70
2199600/2200000 [============================>.] - ETA: 0ss--loss::0.06000--acc::0.9813Epoch 00011: val_loss did not improve
2200000/2200000 [==============================] - 5276s - loss: 0.0600 - acc: 0.9813 - val_loss: 0.0552 - val_acc: 0.9828
Epoch 13/70
2199600/2200000 [============================>.] - ETA: 0ss--loss::0.05988--acc::0.9813Epoch 00012: val_loss did not improve
2200000/2200000 [==============================] - 5272s - loss: 0.0598 - acc: 0.9813 - val_loss: 0.0549 - val_acc: 0.9829
Epoch 14/70
2199600/2200000 [============================>.] - ETA: 0ss--loss::0.05955--acc::0.9814Epoch 00013: val_loss did not improve
2200000/2200000 [==============================] - 5273s - loss: 0.0595 - acc: 0.9814 - val_loss: 0.0553 - val_acc: 0.9829
Epoch 15/70
2199600/2200000 [============================>.] - ETA: 0ss--loss::0.05933--acc::0.9814Epoch 00014: val_loss did not improve
2200000/2200000 [==============================] - 5271s - loss: 0.0593 - acc: 0.9814 - val_loss: 0.0553 - val_acc: 0.9828
Epoch 16/70
2199600/2200000 [============================>.] - ETA: 0ss--loss::0.05911--acc::0.9815Epoch 00015: val_loss did not improve
2200000/2200000 [==============================] - 5271s - loss: 0.0591 - acc: 0.9815 - val_loss: 0.0552 - val_acc: 0.9828
Epoch 00015: early stopping
455024/455024 [==============================] - 326s
[0.059712576528491025, 0.98152178896894904]

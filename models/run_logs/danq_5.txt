
Creating model from DanQ()
Building the model
Compiling model
____________________________________________________________________________________________________
Layer (type)			 Output Shape	       Param #	   Connected to
====================================================================================================
convolution1d_1 (Convolution1D)  (None, 975, 320)      33600	   convolution1d_input_1[0][0]
____________________________________________________________________________________________________
maxpooling1d_1 (MaxPooling1D)	 (None, 75, 320)       0	   convolution1d_1[0][0]
____________________________________________________________________________________________________
dropout_1 (Dropout)		 (None, 75, 320)       0	   maxpooling1d_1[0][0]
____________________________________________________________________________________________________
bidirectional_1 (Bidirectional)  (None, 75, 640)       1640960	   dropout_1[0][0]
____________________________________________________________________________________________________
dropout_2 (Dropout)		 (None, 75, 640)       0	   bidirectional_1[0][0]
____________________________________________________________________________________________________
flatten_1 (Flatten)		 (None, 48000)	       0	   dropout_2[0][0]
____________________________________________________________________________________________________
dense_1 (Dense) 		 (None, 925)	       44400925    flatten_1[0][0]
____________________________________________________________________________________________________
activation_1 (Activation)	 (None, 925)	       0	   dense_1[0][0]
____________________________________________________________________________________________________
dense_2 (Dense) 		 (None, 919)	       850994	   activation_1[0][0]
____________________________________________________________________________________________________
activation_2 (Activation)	 (None, 919)	       0	   dense_2[0][0]
====================================================================================================
Total params: 46,926,479
Trainable params: 46,926,479
Non-trainable params: 0
____________________________________________________________________________________________________
Saving models in json and yaml format to models/json/danq_5.json and  models/yaml/danq_5.yaml
Saving weights to models/weights/danq_5.hdf5 and epoch logs to models/csv/danq_5.csv
Retrieving train, validation, and test data

The date is 02/16/2017
The time is 12:23:13 PM

Loading weights from models/weights/danq_5.hdf5 if it exists
Running at most 70 epochs
Train on 2200000 samples, validate on 8000 samples
Epoch 1/70
2199600/2200000 [============================>.] - ETA: 0ss--loss::0.07688--acc::0.9791Epoch 00000: val_loss improved from inf to 0.06180, saving model to models/weights/danq_5.hdf5
2200000/2200000 [==============================] - 3508s - loss: 0.0768 - acc: 0.9791 - val_loss: 0.0618 - val_acc: 0.9822
Epoch 2/70
2199600/2200000 [============================>.] - ETA: 0ss--loss::0.07088--acc::0.9798Epoch 00001: val_loss improved from 0.06180 to 0.06097, saving model to models/weights/danq_5.hdf5
2200000/2200000 [==============================] - 3507s - loss: 0.0708 - acc: 0.9798 - val_loss: 0.0610 - val_acc: 0.9824
Epoch 3/70
2199600/2200000 [============================>.] - ETA: 0ss--loss::0.06977--acc::0.9799Epoch 00002: val_loss improved from 0.06097 to 0.06001, saving model to models/weights/danq_5.hdf5
2200000/2200000 [==============================] - 3506s - loss: 0.0697 - acc: 0.9799 - val_loss: 0.0600 - val_acc: 0.9824
Epoch 4/70
2199600/2200000 [============================>.] - ETA: 0ss--loss::0.06900--acc::0.9800Epoch 00003: val_loss improved from 0.06001 to 0.05915, saving model to models/weights/danq_5.hdf5
2200000/2200000 [==============================] - 3508s - loss: 0.0690 - acc: 0.9800 - val_loss: 0.0591 - val_acc: 0.9825
Epoch 5/70
2199600/2200000 [============================>.] - ETA: 0ss--loss::0.06866--acc::0.9801Epoch 00004: val_loss improved from 0.05915 to 0.05894, saving model to models/weights/danq_5.hdf5
2200000/2200000 [==============================] - 3507s - loss: 0.0686 - acc: 0.9801 - val_loss: 0.0589 - val_acc: 0.9825
Epoch 6/70
2199600/2200000 [============================>.] - ETA: 0ss--loss::0.06822--acc::0.9801Epoch 00005: val_loss did not improve
2200000/2200000 [==============================] - 3507s - loss: 0.0682 - acc: 0.9801 - val_loss: 0.0591 - val_acc: 0.9823
Epoch 7/70
2199600/2200000 [============================>.] - ETA: 0ss--loss::0.06799--acc::0.9802Epoch 00006: val_loss did not improve
2200000/2200000 [==============================] - 3507s - loss: 0.0679 - acc: 0.9802 - val_loss: 0.0592 - val_acc: 0.9826
Epoch 8/70
2199600/2200000 [============================>.] - ETA: 0ss--loss::0.06766--acc::0.9802Epoch 00007: val_loss did not improve
2200000/2200000 [==============================] - 3508s - loss: 0.0676 - acc: 0.9802 - val_loss: 0.0608 - val_acc: 0.9825
Epoch 9/70
2199600/2200000 [============================>.] - ETA: 0ss--loss::0.06744--acc::0.9803Epoch 00008: val_loss did not improve
2200000/2200000 [==============================] - 3507s - loss: 0.0674 - acc: 0.9803 - val_loss: 0.0591 - val_acc: 0.9827
Epoch 10/70
2199600/2200000 [============================>.] - ETA: 0ss--loss::0.06733--acc::0.9803Epoch 00009: val_loss improved from 0.05894 to 0.05892, saving model to models/weights/danq_5.hdf5
2200000/2200000 [==============================] - 3510s - loss: 0.0673 - acc: 0.9803 - val_loss: 0.0589 - val_acc: 0.9827
Epoch 11/70
2199600/2200000 [============================>.] - ETA: 0ss--loss::0.06722--acc::0.9803Epoch 00010: val_loss did not improve
2200000/2200000 [==============================] - 3508s - loss: 0.0672 - acc: 0.9803 - val_loss: 0.0592 - val_acc: 0.9824
Epoch 12/70
2199600/2200000 [============================>.] - ETA: 0ss--loss::0.06711--acc::0.9803Epoch 00011: val_loss improved from 0.05892 to 0.05871, saving model to models/weights/danq_5.hdf5
2200000/2200000 [==============================] - 3507s - loss: 0.0671 - acc: 0.9803 - val_loss: 0.0587 - val_acc: 0.9827
Epoch 13/70
2199600/2200000 [============================>.] - ETA: 0ss--loss::0.06700--acc::0.9803Epoch 00012: val_loss improved from 0.05871 to 0.05764, saving model to models/weights/danq_5.hdf5
2200000/2200000 [==============================] - 3509s - loss: 0.0670 - acc: 0.9803 - val_loss: 0.0576 - val_acc: 0.9829
Epoch 14/70
2199600/2200000 [============================>.] - ETA: 0ss--loss::0.06699--acc::0.9803Epoch 00013: val_loss did not improve
2200000/2200000 [==============================] - 3508s - loss: 0.0669 - acc: 0.9803 - val_loss: 0.0593 - val_acc: 0.9826
Epoch 15/70
2199600/2200000 [============================>.] - ETA: 0ss--loss::0.06688--acc::0.9803Epoch 00014: val_loss did not improve
2200000/2200000 [==============================] - 3508s - loss: 0.0668 - acc: 0.9803 - val_loss: 0.0581 - val_acc: 0.9827
Epoch 16/70
2199600/2200000 [============================>.] - ETA: 0ss--loss::0.06666--acc::0.9803Epoch 00015: val_loss did not improve
2200000/2200000 [==============================] - 3507s - loss: 0.0666 - acc: 0.9803 - val_loss: 0.0578 - val_acc: 0.9828
Epoch 17/70
2199600/2200000 [============================>.] - ETA: 0ss--loss::0.06644--acc::0.9804Epoch 00016: val_loss did not improve
2200000/2200000 [==============================] - 3507s - loss: 0.0664 - acc: 0.9804 - val_loss: 0.0580 - val_acc: 0.9828
Epoch 18/70
2199600/2200000 [============================>.] - ETA: 0ss--loss::0.06633--acc::0.9804Epoch 00017: val_loss improved from 0.05764 to 0.05756, saving model to models/weights/danq_5.hdf5
2200000/2200000 [==============================] - 3508s - loss: 0.0663 - acc: 0.9804 - val_loss: 0.0576 - val_acc: 0.9828
Epoch 19/70
2199600/2200000 [============================>.] - ETA: 0ss--loss::0.06622--acc::0.9804Epoch 00018: val_loss did not improve
2200000/2200000 [==============================] - 3508s - loss: 0.0662 - acc: 0.9804 - val_loss: 0.0598 - val_acc: 0.9825
Epoch 20/70
2199600/2200000 [============================>.] - ETA: 0ss--loss::0.06611--acc::0.9804Epoch 00019: val_loss did not improve
2200000/2200000 [==============================] - 3508s - loss: 0.0661 - acc: 0.9804 - val_loss: 0.0578 - val_acc: 0.9828
Epoch 21/70
2199600/2200000 [============================>.] - ETA: 0ss--loss::0.06600--acc::0.9804Epoch 00020: val_loss improved from 0.05756 to 0.05739, saving model to models/weights/danq_5.hdf5
2200000/2200000 [==============================] - 3510s - loss: 0.0660 - acc: 0.9804 - val_loss: 0.0574 - val_acc: 0.9828
Epoch 22/70
2199600/2200000 [============================>.] - ETA: 0ss--loss::0.06600--acc::0.9804Epoch 00021: val_loss did not improve
2200000/2200000 [==============================] - 3508s - loss: 0.0660 - acc: 0.9804 - val_loss: 0.0578 - val_acc: 0.9828
Epoch 23/70
2199600/2200000 [============================>.] - ETA: 0ss--loss::0.06599--acc::0.9804Epoch 00022: val_loss did not improve
2200000/2200000 [==============================] - 3509s - loss: 0.0659 - acc: 0.9804 - val_loss: 0.0580 - val_acc: 0.9828
Epoch 24/70
2199600/2200000 [============================>.] - ETA: 0ss--loss::0.06588--acc::0.9804Epoch 00023: val_loss did not improve
2200000/2200000 [==============================] - 3508s - loss: 0.0658 - acc: 0.9804 - val_loss: 0.0591 - val_acc: 0.9828
Epoch 25/70
2199600/2200000 [============================>.] - ETA: 0ss--loss::0.06588--acc::0.9804Epoch 00024: val_loss did not improve
2200000/2200000 [==============================] - 3507s - loss: 0.0658 - acc: 0.9804 - val_loss: 0.0578 - val_acc: 0.9827
Epoch 26/70
2199600/2200000 [============================>.] - ETA: 0ss--loss::0.06588--acc::0.9804Epoch 00025: val_loss did not improve
2200000/2200000 [==============================] - 3507s - loss: 0.0658 - acc: 0.9804 - val_loss: 0.0576 - val_acc: 0.9827
Epoch 27/70
2199600/2200000 [============================>.] - ETA: 0ss--loss::0.06578--acc::0.9805Epoch 00026: val_loss improved from 0.05739 to 0.05718, saving model to models/weights/danq_5.hdf5
2200000/2200000 [==============================] - 3510s - loss: 0.0657 - acc: 0.9805 - val_loss: 0.0572 - val_acc: 0.9827
Epoch 28/70
2199600/2200000 [============================>.] - ETA: 0ss--loss::0.06588--acc::0.9805Epoch 00027: val_loss did not improve
2200000/2200000 [==============================] - 3508s - loss: 0.0658 - acc: 0.9805 - val_loss: 0.0586 - val_acc: 0.9827
Epoch 29/70
2199600/2200000 [============================>.] - ETA: 0ss--loss::0.06577--acc::0.9805Epoch 00028: val_loss did not improve
2200000/2200000 [==============================] - 3507s - loss: 0.0657 - acc: 0.9805 - val_loss: 0.0573 - val_acc: 0.9828
Epoch 30/70
2199600/2200000 [============================>.] - ETA: 0ss--loss::0.06577--acc::0.9805Epoch 00029: val_loss did not improve
2200000/2200000 [==============================] - 3506s - loss: 0.0657 - acc: 0.9805 - val_loss: 0.0588 - val_acc: 0.9827
Epoch 31/70
2199600/2200000 [============================>.] - ETA: 0ss--loss::0.06577--acc::0.9805Epoch 00030: val_loss did not improve
2200000/2200000 [==============================] - 3508s - loss: 0.0657 - acc: 0.9805 - val_loss: 0.0574 - val_acc: 0.9827
Epoch 32/70
2199600/2200000 [============================>.] - ETA: 0ss--loss::0.06588--acc::0.9805Epoch 00031: val_loss did not improve
2200000/2200000 [==============================] - 3508s - loss: 0.0658 - acc: 0.9805 - val_loss: 0.0574 - val_acc: 0.9827
Epoch 33/70
2199600/2200000 [============================>.] - ETA: 0ss--loss::0.06588--acc::0.9805Epoch 00032: val_loss did not improve
2200000/2200000 [==============================] - 3508s - loss: 0.0658 - acc: 0.9805 - val_loss: 0.0586 - val_acc: 0.9827
Epoch 00032: early stopping
455024/455024 [==============================] - 478s 0sss
[0.063315912325212512, 0.98117012606881382]
